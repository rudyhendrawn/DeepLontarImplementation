{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from pipeline.train import Trainer\n",
    "from model_architecture.darknet import YOLO\n",
    "from pipeline.deeplontardataset import visualize_dataset\n",
    "from pipeline.deeplontardataset import DeepLontarDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root_dir = 'C:\\\\Users\\\\rudyh\\\\Documents\\\\Python\\\\datasets\\\\DeepLontar-Dataset'\n",
    "image_dir = os.path.join(data_root_dir, 'images')\n",
    "label_dir = os.path.join(data_root_dir, 'labels')\n",
    "\n",
    "input_shape = (704, 128)\n",
    "dataset = DeepLontarDataset(image_dir=image_dir, label_dir=label_dir)\n",
    "train_set, test_set = dataset.split_dataset(split_ratio=0.8)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=8, shuffle=True, num_workers=4)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=8, shuffle=False, num_workers=4)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\"\"\"\n",
    "An anchor box is a bounding box with various scales and aspect ratios\n",
    "centered on each pixel. The anchor box samples a large number of regions \n",
    "in the input image and determines whether these regions contain objects of \n",
    "interest and adjusts the region boundaries to predict with better accuracy \n",
    "the ground truth bounding box of the object. \n",
    "Other parameters also come from below paper.\n",
    "Paper: https://ieeexplore.ieee.org/document/9694598\n",
    "\"\"\"\n",
    "anchors = [[37, 49], [65, 3], [46, 75], [63, 58], [68, 85], [95, 73], [96, 105], [127, 303], [294, 137]]\n",
    "\n",
    "momentum = 0.949\n",
    "num_classes = 55\n",
    "decay = 5e-4\n",
    "\n",
    "# Define model\n",
    "batch_size = 4\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 3\n",
    "\n",
    "# Data Transformation\n",
    "data_transform = torchvision.transforms.Compose([\n",
    "\ttorchvision.transforms.ToTensor(),\n",
    "\ttorchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 37  49]\n",
      " [ 65   3]\n",
      " [ 46  75]\n",
      " [ 63  58]\n",
      " [ 68  85]\n",
      " [ 95  73]\n",
      " [ 96 105]\n",
      " [127 303]\n",
      " [294 137]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "anchors = [[37, 49], [65, 3], [46, 75], [63, 58], [68, 85], [95, 73], [96, 105], [127, 303], [294, 137]]\n",
    "print(np.array(anchors).reshape((-1, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(22, 4), (44, 8), (88, 16)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(input_shape[0] // s, input_shape[1] // s) for s in [32, 16, 8]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "beadf95f16862a7393e5fa829bfbfa3e6e0a11534280b92e7d2b4d010105fa65"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
